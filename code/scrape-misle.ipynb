{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activity_list = ['3673761', '3662467', '3669435', '3662636', '3659777', '3664756', '3663135', '3662547']\n",
    "activity_data = pd.read_excel('../data/misle/MISLE Incident Investigations DT.xlsx')\n",
    "activity_list = activity_data['Activity ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3393\n"
     ]
    }
   ],
   "source": [
    "print(len(activity_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    activity_id                                     incident_brief\n",
      "0       3743747  At approximately 1925 on May 11th, 2010 the M/...\n",
      "1       3748349  UTV Ocie Clark had a buoy stuck in the starboa...\n",
      "2       3709328  M/V Jean Akin reported alliding with the Sherm...\n",
      "3       3720297  Campbell transportation advised the USCG that ...\n",
      "4       3697617  On 16Mar2010 at 2230L, the UTV SANDY DRAKE was...\n",
      "..          ...                                                ...\n",
      "73      3757646  Received a report of the UTV James H. Hunter a...\n",
      "74      3766718  While sliding barges on board the M/V Sally Br...\n",
      "75      3795297  05 MAY 2010, The M/V GINNY STONE caught an obj...\n",
      "76      3732642  UTV Hazel (Crounse Corp) allided with the Clar...\n",
      "77      3744432  Pikes Island L/D operator advised the UTV MONG...\n",
      "\n",
      "[78 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "misle_scrape = pd.read_json('../data/misle/scrape/misle-scraped-brief.json')\n",
    "\n",
    "print(misle_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(cssID, soup):\n",
    "    data = soup.find(id=cssID)\n",
    "    if(data is not None):\n",
    "        return data.text #to extract the text without html tags\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "briefs = []\n",
    "\n",
    "class MISLEViewStateSpider(scrapy.Spider):\n",
    "    name = 'misle-viewstate'\n",
    "    start_urls = ['https://cgmix.uscg.mil/IIR/IIRSearch.aspx']\n",
    "    download_delay = 1.5\n",
    "    \n",
    "    def __init__(self, activity_id=None):\n",
    "        self.activity_id = activity_id\n",
    "    \n",
    "    def parse(self, response):\n",
    "        yield scrapy.FormRequest('https://cgmix.uscg.mil/IIR/IIRSearch.aspx',\n",
    "                                 formdata={'__EVENTVALIDATION': response.css('input#__EVENTVALIDATION::attr(value)'\n",
    "                                                                      ).extract_first(),\n",
    "                                           'TextBoxActivityNumber': self.activity_id,\n",
    "                                           'DropDownListVesselService':'ALL',\n",
    "                                           'TextBoxFromDate':'01/01/2010',\n",
    "                                           'TextBoxToDate':'10/16/2019',\n",
    "                                           'ButtonSearch':'Search',\n",
    "                                           '__VIEWSTATE': response.css('input#__VIEWSTATE::attr(value)'\n",
    "                                                                      ).extract_first()\n",
    "                                          },\n",
    "                                 callback=self.parse_activity)\n",
    "\n",
    "    def parse_activity(self, response):\n",
    "        yield scrapy.FormRequest('https://cgmix.uscg.mil/IIR/IIRSearch.aspx',\n",
    "                                 formdata={'__EVENTVALIDATION': response.css('input#__EVENTVALIDATION::attr(value)'\n",
    "                                                                      ).extract_first(),\n",
    "                                           '__VIEWSTATEGENERATOR': response.css('input#__VIEWSTATEGENERATOR::attr(value)'\n",
    "                                                                      ).extract_first(),\n",
    "                                           '__EVENTTARGET':'GridViewIIR$ctl02$ReportButton',\n",
    "                                           '__VIEWSTATE': response.css('input#__VIEWSTATE::attr(value)'\n",
    "                                                                      ).extract_first()\n",
    "                                          },\n",
    "                                 callback=self.parse_results)\n",
    "\n",
    "    def parse_results(self, response):\n",
    "        soup = BeautifulSoup(response.body, 'html.parser')\n",
    "        brief_result = {\n",
    "            'activity_id': soup.find(id='LabelActivityNumber').text,\n",
    "            'incident_brief': soup.find(id='LabelIncidentBrief').text\n",
    "        }\n",
    "        briefs.append(brief_result)\n",
    "        \n",
    "        yield brief_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(settings={\n",
    "    'FEED_FORMAT':'json',\n",
    "    'FEED_URI': '../data/misle/scrape/misle-scraped-brief.json',\n",
    "    'LOG_LEVEL': logging.WARNING,\n",
    "})\n",
    "\n",
    "for i in range(len(activity_list)):\n",
    "    if i < 100:\n",
    "        time.sleep(5)\n",
    "        process.crawl(MISLEViewStateSpider, activity_list[i])\n",
    "    \n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "briefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ['J.K. Rowling']\n",
    "tags = ['live-death-love','friends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpidyQuotesViewStateSpider(scrapy.Spider):\n",
    "    name = 'spidyquotes-viewstate'\n",
    "    start_urls = ['http://quotes.toscrape.com/search.aspx']\n",
    "    download_delay = 1.5\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for author in authors:\n",
    "            yield scrapy.FormRequest('http://quotes.toscrape.com/filter.aspx',\n",
    "                                     formdata={'author': author,\n",
    "                                               '__VIEWSTATE': response.css('input#__VIEWSTATE::attr(value)'\n",
    "                                                                          ).extract_first()\n",
    "                                              },\n",
    "                                     callback=self.parse_tags)\n",
    "\n",
    "    def parse_tags(self, response):\n",
    "        for tag in tags:\n",
    "            yield scrapy.FormRequest('http://quotes.toscrape.com/filter.aspx',\n",
    "                                     formdata={'author': response.css('select#author > option[selected] ::attr(value)'\n",
    "                                                                     ).extract_first(),\n",
    "                                               'tag': tag,\n",
    "                                               '__VIEWSTATE': response.css('input#__VIEWSTATE::attr(value)'\n",
    "                                                                          ).extract_first()\n",
    "                                              },\n",
    "                                     callback=self.parse_results)\n",
    "\n",
    "    def parse_results(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                'quote': quote.css('span.content ::text').extract_first(),\n",
    "                'author': quote.css('span.author ::text').extract_first(),\n",
    "                'tag': quote.css('span.tag ::text').extract_first(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(settings={\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'items.json'\n",
    "})\n",
    "\n",
    "process.crawl(SpidyQuotesViewStateSpider)\n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2019-10-16 15:42:25 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
    "2019-10-16 15:42:25 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Sep 11 2019, 09:35:55) - [Clang 8.0.0 (clang-800.0.42.1)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Darwin-15.6.0-x86_64-i386-64bit\n",
    "2019-10-16 15:42:25 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'json', 'FEED_URI': 'items.json'}\n",
    "2019-10-16 15:42:25 [scrapy.extensions.telnet] INFO: Telnet Password: a9d3096c91063f7f\n",
    "2019-10-16 15:42:25 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage',\n",
    " 'scrapy.extensions.feedexport.FeedExporter',\n",
    " 'scrapy.extensions.logstats.LogStats']\n",
    "2019-10-16 15:42:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2019-10-16 15:42:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2019-10-16 15:42:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2019-10-16 15:42:25 [scrapy.core.engine] INFO: Spider opened\n",
    "2019-10-16 15:42:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2019-10-16 15:42:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2019-10-16 15:42:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/search.aspx> (referer: None)\n",
    "2019-10-16 15:42:27 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://quotes.toscrape.com/filter.aspx> (referer: http://quotes.toscrape.com/search.aspx)\n",
    "2019-10-16 15:42:28 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://quotes.toscrape.com/filter.aspx> (referer: http://quotes.toscrape.com/filter.aspx)\n",
    "2019-10-16 15:42:28 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/filter.aspx>\n",
    "{'quote': '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”', 'author': 'J.K. Rowling', 'tag': 'friends'}\n",
    "2019-10-16 15:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://quotes.toscrape.com/filter.aspx> (referer: http://quotes.toscrape.com/filter.aspx)\n",
    "2019-10-16 15:42:29 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/filter.aspx>\n",
    "{'quote': '“Do not pity the dead, Harry. Pity the living, and, above all those who live without love.”', 'author': 'J.K. Rowling', 'tag': 'live-death-love'}\n",
    "2019-10-16 15:42:29 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2019-10-16 15:42:29 [scrapy.extensions.feedexport] INFO: Stored json feed (2 items) in: items.json\n",
    "2019-10-16 15:42:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 4437,\n",
    " 'downloader/request_count': 4,\n",
    " 'downloader/request_method_count/GET': 1,\n",
    " 'downloader/request_method_count/POST': 3,\n",
    " 'downloader/response_bytes': 11386,\n",
    " 'downloader/response_count': 4,\n",
    " 'downloader/response_status_count/200': 4,\n",
    " 'elapsed_time_seconds': 4.152423,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2019, 10, 16, 19, 42, 29, 780676),\n",
    " 'item_scraped_count': 2,\n",
    " 'log_count/DEBUG': 6,\n",
    " 'log_count/INFO': 11,\n",
    " 'memusage/max': 99852288,\n",
    " 'memusage/startup': 99852288,\n",
    " 'request_depth_max': 2,\n",
    " 'response_received_count': 4,\n",
    " 'scheduler/dequeued': 4,\n",
    " 'scheduler/dequeued/memory': 4,\n",
    " 'scheduler/enqueued': 4,\n",
    " 'scheduler/enqueued/memory': 4,\n",
    " 'start_time': datetime.datetime(2019, 10, 16, 19, 42, 25, 628253)}\n",
    "2019-10-16 15:42:29 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
